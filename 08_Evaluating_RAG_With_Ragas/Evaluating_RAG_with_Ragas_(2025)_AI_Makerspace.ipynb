{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLokDKoN1aKv"
   },
   "source": [
    "# Using Ragas to Evaluate a RAG Application built with LangChain and LangGraph\n",
    "\n",
    "In the following notebook, we'll be looking at how [Ragas](https://github.com/explodinggradients/ragas) can be helpful in a number of ways when looking to evaluate your RAG applications!\n",
    "\n",
    "While this example is rooted in LangChain/LangGraph - Ragas is framework agnostic (you don't even need to be using a framework!).\n",
    "\n",
    "- 🤝 Breakout Room #1\n",
    "  1. Task 1: Installing Required Libraries\n",
    "  2. Task 2: Set Environment Variables\n",
    "  3. Task 3: Synthetic Dataset Generation for Evaluation using Ragas\n",
    "  4. Task 4: Evaluating our Pipeline with Ragas\n",
    "  5. Task 6: Making Adjustments and Re-Evaluating\n",
    "\n",
    "But first! Let's set some dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0k9tqHdq2BGi"
   },
   "source": [
    "## Dependencies and API Keys:\n",
    "\n",
    "> NOTE: Please skip the pip install commands if you are running the notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU ragas==0.2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3d5seTX2xyx"
   },
   "source": [
    "We'll also need to provide our API keys.\n",
    "\n",
    "First, OpenAI's for our LLM/embedding model combination!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Please enter your OpenAI API key!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zTz1-4U3Hg0"
   },
   "source": [
    "## Generating Synthetic Test Data\n",
    "\n",
    "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
    "\n",
    "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssK40Eh4MIc"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
    "\n",
    "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
    "\n",
    "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
    "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
    "\n",
    "Let's start by collecting our data into a useful pile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 31524    0 31524    0     0   188k      0 --:--:-- --:--:-- --:--:--  188k\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 70549    0 70549    0     0   114k      0 --:--:-- --:--:-- --:--:--  114k\n"
     ]
    }
   ],
   "source": [
    "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfKQgiR14Omj"
   },
   "source": [
    "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfyA65MM4Tbn"
   },
   "source": [
    "### Knowledge Graph Based Synthetic Generation\n",
    "\n",
    "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
    "\n",
    "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbZ9j1EL-X55"
   },
   "source": [
    "### Abstracted SDG\n",
    "\n",
    "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
    "\n",
    "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcf13d7989a4bc7b716ac2ca1cbdd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8011fe398cfb47e2b07446d21ba297e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd08479e7b3461ab8ca1c241ebee4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b728e48d1e364ad48c4bd8aa54185d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b987851e93094bff9d9d5cc3e35da4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b27b986a2845cebc13f953cd267274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf7ef08c6f74e68882d01669b15261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee21369861455b85caf58eeaa51535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3e495038bf4e2eaa2e0e3ad76a5d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which organizations, including Anthropic, have...</td>\n",
       "      <td>[We don’t yet know how to build GPT-4 Vibes Ba...</td>\n",
       "      <td>A year ago, the only organization that had rel...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some challenges associated with evalu...</td>\n",
       "      <td>[I’m surprised that no-one has beaten the now ...</td>\n",
       "      <td>Evaluating GPT-4 is challenging because there ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is AI mean now, is it just LLMs or more, ...</td>\n",
       "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
       "      <td>AI now often refers to Large Language Models (...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me AI product manager, i look at analytics, i ...</td>\n",
       "      <td>[Microsoft over this issue. The 69 page PDF is...</td>\n",
       "      <td>In the context provided, Plausible refers to P...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key advantages of using synthetic...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>Synthetic training data offers several direct ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Given the rapid advancements in large language...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>The context reveals that in 2024, significant ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how ai model trainin get more better for envir...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>ai model trainin got better for environment bu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are some of the ethical and environmental...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>The training and deployment of large language ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Why people keep using ChatGPT even though it s...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nI’m surprised that no-one has beat...</td>\n",
       "      <td>People keep using ChatGPT even though it somet...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Considering the dramatic reduction in LLM pric...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\non a story about the town's histor...</td>\n",
       "      <td>Google’s Gemini 1.5 Flash 8B model exemplifies...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are the key innovations introduced by Gem...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>Gemini 1.5 Pro, also known as Gemini Pro 1.5, ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Why people say ChatGPT is both smart and dumb,...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nI’m surprised that no-one has beat...</td>\n",
       "      <td>People say ChatGPT is both smart and dumb beca...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   Which organizations, including Anthropic, have...   \n",
       "1   What are some challenges associated with evalu...   \n",
       "2   what is AI mean now, is it just LLMs or more, ...   \n",
       "3   me AI product manager, i look at analytics, i ...   \n",
       "4   What are the key advantages of using synthetic...   \n",
       "5   Given the rapid advancements in large language...   \n",
       "6   how ai model trainin get more better for envir...   \n",
       "7   What are some of the ethical and environmental...   \n",
       "8   Why people keep using ChatGPT even though it s...   \n",
       "9   Considering the dramatic reduction in LLM pric...   \n",
       "10  What are the key innovations introduced by Gem...   \n",
       "11  Why people say ChatGPT is both smart and dumb,...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [We don’t yet know how to build GPT-4 Vibes Ba...   \n",
       "1   [I’m surprised that no-one has beaten the now ...   \n",
       "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
       "3   [Microsoft over this issue. The 69 page PDF is...   \n",
       "4   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "5   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "6   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "7   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "8   [<1-hop>\\n\\nI’m surprised that no-one has beat...   \n",
       "9   [<1-hop>\\n\\non a story about the town's histor...   \n",
       "10  [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "11  [<1-hop>\\n\\nI’m surprised that no-one has beat...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   A year ago, the only organization that had rel...   \n",
       "1   Evaluating GPT-4 is challenging because there ...   \n",
       "2   AI now often refers to Large Language Models (...   \n",
       "3   In the context provided, Plausible refers to P...   \n",
       "4   Synthetic training data offers several direct ...   \n",
       "5   The context reveals that in 2024, significant ...   \n",
       "6   ai model trainin got better for environment bu...   \n",
       "7   The training and deployment of large language ...   \n",
       "8   People keep using ChatGPT even though it somet...   \n",
       "9   Google’s Gemini 1.5 Flash 8B model exemplifies...   \n",
       "10  Gemini 1.5 Pro, also known as Gemini Pro 1.5, ...   \n",
       "11  People say ChatGPT is both smart and dumb beca...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Testset(samples=[TestsetSample(eval_sample=SingleTurnSample(user_input='Which organizations, including Anthropic, have produced large language models that are considered better-than-GPT-3 class, and how has the landscape changed since only OpenAI had released a generally useful LLM?', retrieved_contexts=None, reference_contexts=['We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still,'], response=None, multi_responses=None, reference='A year ago, the only organization that had released a generally useful LLM was OpenAI. Since then, better-than-GPT-3 class models have been produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu, and a number of other organizations. This marks a significant shift in the landscape, as the development and release of advanced LLMs are no longer limited to a single organization.', rubrics=None), synthesizer_name='single_hop_specifc_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='What are some challenges associated with evaluating GPT-4?', retrieved_contexts=None, reference_contexts=['I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and'], response=None, multi_responses=None, reference=\"Evaluating GPT-4 is challenging because there are plenty of benchmarks, but no benchmark can determine if an LLM like GPT-4 actually 'feels' right for a given task. It often requires working with the model for weeks to develop an intuition for its strengths and weaknesses, which limits how many models can be evaluated personally. Additionally, there is no good methodology for understanding the impact of prompt tweaks, making the process feel like 'Vibes Based Development.'\", rubrics=None), synthesizer_name='single_hop_specifc_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='what is AI mean now, is it just LLMs or more, how easy is AI now?', retrieved_contexts=None, reference_contexts=['Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models'], response=None, multi_responses=None, reference='AI now often refers to Large Language Models (LLMs), which are the latest and most interesting development in the academic field of Artificial Intelligence. In 2023, it became clear that LLMs are actually quite easy to build, and you can run them on your own devices. Hobbyists can also build their own fine-tuned models.', rubrics=None), synthesizer_name='single_hop_specifc_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='me AI product manager, i look at analytics, i see Plausible, what Plausible mean here and how it used for blog traffic, can you explain with details from context?', retrieved_contexts=None, reference_contexts=[\"Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): Tag cloud words in order of size: ai, generativeai, llms, openai, chatgpt, projects, python, datasette, ethics, llama, homebrewllms, sqlite, gpt3, promptengineering, promptinjection, llm, security, opensource, gpt4, weeknotes The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 105 ai 1241 generative-ai 1068 llms 1057 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\"], response=None, multi_responses=None, reference='In the context provided, Plausible refers to Plausible analytics, which is a tool used to track and analyze website traffic. The author mentions grabbing a screenshot of their Plausible analytics for the year, feeding that to ChatGPT Vision, and then extracting the data into a table. This process allowed them to see their top blog entries by amount of traffic, including metrics like visitors and pageviews for each article. So, Plausible is used here to monitor and report on the performance of blog content in terms of how many people visited and viewed each post.', rubrics=None), synthesizer_name='single_hop_specifc_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='What are the key advantages of using synthetic training data over organic data in large language model development, and how have recent high-performing models like Meta’s Llama 3.3 70B and DeepSeek v3 leveraged synthetic data to improve their capabilities?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nThe rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. Screenshot of a comparison table showing AI model rankings. Table headers: Rank (UB), Rank (StyleCtrl), Model, Arena Score, 95% CI, Votes, Organization, License. Shows 12 models including GLM-4-0520, Llama-3-70B-Instruct, Gemini-1.5-Flash-8B-Exp-0827, with rankings, scores, and licensing details. Models range from rank 52-69 with Arena scores between 1186-1207. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: MLC Chat: Llama - [System] Ready to chat. a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist. Show as Markdown is turned on. Here\\'s a plot outline for a Netflix Christmas movie: Title: \"Love in the Clay\" Plot Outline: We meet our protagonist, JESSICA, a data journalist who has just returned to her hometown of Willow Creek, a small, charming town nestled in the snow-covered mountains. She\\'s back to work', '<2-hop>\\n\\nof the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot'], response=None, multi_responses=None, reference=\"Synthetic training data offers several direct advantages over organic data in large language model (LLM) development. According to the Phi-4 technical report, synthetic data enables structured and gradual learning, as each token generated by a language model is predicted by the preceding tokens, making it easier for the model to follow reasoning patterns. In contrast, organic datasets often have complex and indirect relationships between tokens, which can make next-token prediction more challenging for the model to learn effectively. Additionally, synthetic data allows for careful design and curation of training examples, moving away from the previous practice of indiscriminately scraping the web for organic data.\\n\\nRecent high-performing models have leveraged these advantages. For example, DeepSeek v3 used 'reasoning' data created by its larger model, DeepSeek-R1, to enhance its training process. Similarly, Meta’s Llama 3.3 70B was fine-tuned using over 25 million synthetically generated examples. This approach of using synthetic data, often generated by larger models to train smaller or more efficient alternatives, has become increasingly common and is considered a key factor in the improved capabilities and efficiency of modern LLMs.\", rubrics=None), synthesizer_name='multi_hop_abstract_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Given the rapid advancements in large language model (LLM) capabilities and efficiency described in 2024, how have these developments influenced the environmental impact of AI model training, and what ethical and environmental considerations are highlighted in the context of deploying such models, especially as they become more accessible for local use?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nThe rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. Screenshot of a comparison table showing AI model rankings. Table headers: Rank (UB), Rank (StyleCtrl), Model, Arena Score, 95% CI, Votes, Organization, License. Shows 12 models including GLM-4-0520, Llama-3-70B-Instruct, Gemini-1.5-Flash-8B-Exp-0827, with rankings, scores, and licensing details. Models range from rank 52-69 with Arena scores between 1186-1207. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: MLC Chat: Llama - [System] Ready to chat. a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist. Show as Markdown is turned on. Here\\'s a plot outline for a Netflix Christmas movie: Title: \"Love in the Clay\" Plot Outline: We meet our protagonist, JESSICA, a data journalist who has just returned to her hometown of Willow Creek, a small, charming town nestled in the snow-covered mountains. She\\'s back to work', \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"], response=None, multi_responses=None, reference=\"The context reveals that in 2024, significant progress was made in both the capabilities and efficiency of large language models (LLMs). Models that previously required datacenter-class servers and expensive GPUs can now be run on consumer hardware, such as a 64GB M2 MacBook Pro or even an iPhone, due to improvements in model efficiency and inference performance. This shift suggests that the environmental impact of running LLMs for inference has improved, as less powerful and more widely available hardware can now support advanced models. However, the context also notes that the environmental impact of AI model training has both 'got better' and 'got much, much worse,' indicating a complex picture: while efficiency gains reduce the resources needed for inference, the overall scale and frequency of model training and deployment have increased, potentially exacerbating environmental concerns. Ethically, the text emphasizes the importance of critical evaluation and responsible deployment of AI, cautioning against both uncritical hype and blanket condemnation. It argues that while there is genuine value in LLMs, those with expertise have a duty to guide others in making informed decisions that balance the benefits of AI with its ethical and environmental costs. Thus, as LLMs become more accessible and their use proliferates, ongoing attention to both the environmental impact of training and the broader ethical considerations of AI deployment remains essential.\", rubrics=None), synthesizer_name='multi_hop_abstract_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='how ai model trainin get more better for environment but also more worse, and what ethical and environmental things should ai product manager think about when makin decision about llms?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nThe rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. Screenshot of a comparison table showing AI model rankings. Table headers: Rank (UB), Rank (StyleCtrl), Model, Arena Score, 95% CI, Votes, Organization, License. Shows 12 models including GLM-4-0520, Llama-3-70B-Instruct, Gemini-1.5-Flash-8B-Exp-0827, with rankings, scores, and licensing details. Models range from rank 52-69 with Arena scores between 1186-1207. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: MLC Chat: Llama - [System] Ready to chat. a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist. Show as Markdown is turned on. Here\\'s a plot outline for a Netflix Christmas movie: Title: \"Love in the Clay\" Plot Outline: We meet our protagonist, JESSICA, a data journalist who has just returned to her hometown of Willow Creek, a small, charming town nestled in the snow-covered mountains. She\\'s back to work', \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"], response=None, multi_responses=None, reference=\"ai model trainin got better for environment but also much, much worse. this because new models like gpt-4 class can now run on regular laptops, showin big gains in efficiency and less need for huge datacenter servers, which help environment. but at same time, more organizations trainin big models, and longer context lengths mean more compute and energy use, so overall impact can get worse. ethical and environmental things ai product manager should think about is not just callin ai 'environmentally catastrophic plagiarism machines', but help people understand real value and guide them to use llms in good ways, avoidin bad decisions and traps. bein critical and explainin both good applications and risks is important for makin better choices about ai.\", rubrics=None), synthesizer_name='multi_hop_abstract_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='What are some of the ethical and environmental considerations related to the training and deployment of large language models, and how has the environmental impact of AI model training changed as model efficiency has improved?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nThe rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. Screenshot of a comparison table showing AI model rankings. Table headers: Rank (UB), Rank (StyleCtrl), Model, Arena Score, 95% CI, Votes, Organization, License. Shows 12 models including GLM-4-0520, Llama-3-70B-Instruct, Gemini-1.5-Flash-8B-Exp-0827, with rankings, scores, and licensing details. Models range from rank 52-69 with Arena scores between 1186-1207. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: MLC Chat: Llama - [System] Ready to chat. a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist. Show as Markdown is turned on. Here\\'s a plot outline for a Netflix Christmas movie: Title: \"Love in the Clay\" Plot Outline: We meet our protagonist, JESSICA, a data journalist who has just returned to her hometown of Willow Creek, a small, charming town nestled in the snow-covered mountains. She\\'s back to work', \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"], response=None, multi_responses=None, reference='The training and deployment of large language models (LLMs) raise important ethical and environmental considerations. One key concern is the environmental impact of training these models, which can be significant due to the computational resources required. However, recent advancements have led to notable improvements in model efficiency, resulting in better training and inference performance. For example, models that previously required datacenter-class servers can now run on consumer hardware like a 64GB M2 MacBook Pro, demonstrating substantial gains in efficiency. Despite these improvements, the text notes that the environmental impact of AI model training has both improved and worsened: while efficiency gains have reduced the resources needed for inference, the overall scale and frequency of model training have increased, potentially offsetting these benefits. Ethically, it is important to recognize both the genuine value and the potential harms of LLMs, and to provide guidance to decision-makers to avoid unintuitive pitfalls. Overstating the negative environmental impact without acknowledging the progress and value of these models can be misleading, so a balanced and critical approach is necessary.', rubrics=None), synthesizer_name='multi_hop_abstract_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Why people keep using ChatGPT even though it sometimes act dumb or gullible, and what are the main problems with evaluating its performance and ethical issues around its use, especially when it comes to things like code generation and the legal challenges mentioned in the Microsoft and OpenAI lawsuit?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nI’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and', \"<2-hop>\\n\\nMicrosoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): Tag cloud words in order of size: ai, generativeai, llms, openai, chatgpt, projects, python, datasette, ethics, llama, homebrewllms, sqlite, gpt3, promptengineering, promptinjection, llm, security, opensource, gpt4, weeknotes The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 105 ai 1241 generative-ai 1068 llms 1057 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\"], response=None, multi_responses=None, reference=\"People keep using ChatGPT even though it sometimes acts dumb or gullible because, despite its unpredictable behavior and occasional need for odd prompting tricks (like capitalizing words or giving positive reinforcement), it remains highly capable—especially at tasks like code generation. The grammar of programming languages is simpler than natural languages, and with tools like ChatGPT Code Interpreter, the model can run and correct its own code, making hallucinations less of a problem for code than for other tasks. However, evaluating ChatGPT’s performance is challenging: benchmarks don’t capture the 'feel' of using the model, and it often takes weeks of hands-on use to understand its strengths and weaknesses. This makes it hard for product managers and engineers to systematically compare models. On the ethical and legal side, there are major unresolved issues, such as the use of unlicensed data for training, which has led to lawsuits like the one from the New York Times against Microsoft and OpenAI. These legal battles raise questions about whether it’s acceptable to train models on people’s content without permission, especially when those models compete with the original creators. The outcome of such cases is expected to have a profound impact on the future development and deployment of technologies like ChatGPT.\", rubrics=None), synthesizer_name='multi_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Considering the dramatic reduction in LLM pricing and the rise of multi-modal capabilities, how has Google’s Gemini 1.5 Flash 8B model exemplified these industry trends, and what implications does this have for user accessibility and the broader competitive landscape among large language model providers?', retrieved_contexts=None, reference_contexts=[\"<1-hop>\\n\\non a story about the town's history and the effects of gentrification on the local community. Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—200x cheaper than GPT-4, nearly 7x cheaper than GPT-3.5 and massively more capable than that model. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: llm -m gemini-1.5-flash-8b-latest describe -a IMG_1825.jpeg Against this photo of butterflies at the California Academy of Sciences: A photo of two butterflies feeding on a red tray A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but\", \"<2-hop>\\n\\nMicrosoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): Tag cloud words in order of size: ai, generativeai, llms, openai, chatgpt, projects, python, datasette, ethics, llama, homebrewllms, sqlite, gpt3, promptengineering, promptinjection, llm, security, opensource, gpt4, weeknotes The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 105 ai 1241 generative-ai 1068 llms 1057 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\"], response=None, multi_responses=None, reference='Google’s Gemini 1.5 Flash 8B model exemplifies two major industry trends: the significant drop in LLM pricing and the rapid advancement of multi-modal capabilities. According to the context, Gemini 1.5 Flash 8B is priced at $0.0375 per million tokens, making it 27 times cheaper than GPT-3.5 Turbo from the previous year and one of the most affordable options among leading providers. This dramatic reduction in cost is attributed to increased competition and improved efficiency across the industry, which not only lowers the financial barrier for users but also reduces the environmental impact per prompt. Furthermore, Gemini 1.5 Flash 8B supports multi-modal input, allowing users to process images alongside text, as demonstrated by generating detailed descriptions for thousands of photos at a negligible cost. This accessibility to advanced, multi-modal AI at low prices broadens the potential user base and intensifies competition among providers like OpenAI, Anthropic, and Meta, all of whom are rapidly releasing similar capabilities. As a result, Google’s approach with Gemini 1.5 Flash 8B both reflects and accelerates the industry’s shift toward more affordable, capable, and accessible AI tools.', rubrics=None), synthesizer_name='multi_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='What are the key innovations introduced by Gemini 1.5 Pro (also referred to as Gemini Pro 1.5), and how did these features impact the landscape of large language models in 2024 according to industry evaluations?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nThe rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. Screenshot of a comparison table showing AI model rankings. Table headers: Rank (UB), Rank (StyleCtrl), Model, Arena Score, 95% CI, Votes, Organization, License. Shows 12 models including GLM-4-0520, Llama-3-70B-Instruct, Gemini-1.5-Flash-8B-Exp-0827, with rankings, scores, and licensing details. Models range from rank 52-69 with Arena scores between 1186-1207. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: MLC Chat: Llama - [System] Ready to chat. a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist. Show as Markdown is turned on. Here\\'s a plot outline for a Netflix Christmas movie: Title: \"Love in the Clay\" Plot Outline: We meet our protagonist, JESSICA, a data journalist who has just returned to her hometown of Willow Creek, a small, charming town nestled in the snow-covered mountains. She\\'s back to work', \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"], response=None, multi_responses=None, reference=\"Gemini 1.5 Pro, also known as Gemini Pro 1.5, was released by Google in February 2024 and marked a significant advancement in large language models. Its key innovations included producing GPT-4 level outputs and introducing a 1 million (later 2 million) token input context length, as well as the ability to input video. These features enabled new use-cases, such as analyzing entire books or large codebases, and made long-input tasks much more feasible. The model's release illustrated the 2024 trend of increased context lengths, with every major provider subsequently offering models with 100,000+ token capacities. According to the Chatbot Arena Leaderboard, Gemini 1.5 Pro was among the earliest models to surpass the original GPT-4 in ranking, contributing to a shift where 18 organizations had models outperforming GPT-4 by the end of 2024. This comprehensive improvement in model capabilities and context handling redefined industry standards and expanded the practical applications of LLMs.\", rubrics=None), synthesizer_name='multi_hop_specific_query_synthesizer'), TestsetSample(eval_sample=SingleTurnSample(user_input='Why people say ChatGPT is both smart and dumb, and how does that make it hard to know if it good for coding or not, and what problems does this cause for people trying to use ChatGPT for real work?', retrieved_contexts=None, reference_contexts=['<1-hop>\\n\\nI’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and', \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"], response=None, multi_responses=None, reference='People say ChatGPT is both smart and dumb because sometimes it does things nobody expected, which is fun, but other times you have to do silly things to get it to work right, like offering it cash tips or telling it your career depends on it. This makes it hard to know if ChatGPT is good for coding or not because it can hallucinate code that doesn’t exist, but for code, you can run it and see if it works, so hallucination is less of a problem. Still, evaluating ChatGPT is hard because you have to use it for weeks to get a feel for its strengths and weaknesses, and there’s no good way to know if small prompt changes really help. This causes problems for people trying to use ChatGPT for real work because it’s hard to control, it’s gullible (believes what you tell it), and there’s no robust way to stop prompt injection or other security issues. So, while ChatGPT can be very useful, especially for code, it’s also unpredictable and needs careful handling to avoid mistakes.', rubrics=None), synthesizer_name='multi_hop_specific_query_synthesizer')])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1-hop>\\n\\nI’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and',\n",
       " \"<2-hop>\\n\\nof very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon, Bluesky, Twitter or subscribe to my newsletter More recent articles Watching o3 guess a photo's location is surreal, dystopian and wildly entertaining - 26th April 2025 Exploring Promptfoo via Dave Guarino's SNAP evals - 24th April 2025 AI assisted search-based research actually works now - 21st April 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 360 ai 1241 openai 284 generative-ai 1068 local-llms 101 llms 1057 anthropic 135 gemini 80 meta 31 llm-reasoning 42 long-context 15 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().iloc[11]['reference_contexts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People say ChatGPT is both smart and dumb because sometimes it does things nobody expected, which is fun, but other times you have to do silly things to get it to work right, like offering it cash tips or telling it your career depends on it. This makes it hard to know if ChatGPT is good for coding or not because it can hallucinate code that doesn’t exist, but for code, you can run it and see if it works, so hallucination is less of a problem. Still, evaluating ChatGPT is hard because you have to use it for weeks to get a feel for its strengths and weaknesses, and there’s no good way to know if small prompt changes really help. This causes problems for people trying to use ChatGPT for real work because it’s hard to control, it’s gullible (believes what you tell it), and there’s no robust way to stop prompt injection or other security issues. So, while ChatGPT can be very useful, especially for code, it’s also unpredictable and needs careful handling to avoid mistakes.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().iloc[11]['reference']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ni4Q14_arJYw"
   },
   "source": [
    "## LangChain RAG\n",
    "\n",
    "Now we'll construct our LangChain RAG, which we will be evaluating using the above created test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGy99jkVrVqX"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "Let's start with building our retrieval pipeline, which will involve loading the same data we used to create our synthetic test set above.\n",
    "\n",
    "> NOTE: We need to use the same data - as our test set is specifically designed for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob=\"*.html\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQv3Psil_D2A"
   },
   "source": [
    "Now that we have our data loaded, let's split it into chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question: \n",
    "\n",
    "What is the purpose of the `chunk_overlap` parameter in the `RecursiveCharacterTextSplitter`?\n",
    "\n",
    "1 - Preserve context across chunks.\n",
    "\n",
    "2 - Avoid cutting off important information that appears at the boundary between two chunks.\n",
    "\n",
    "3 - Improve downstream tasks like document retrieval, summarization, or question answering, where maintaining context is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EcbmBBC_G2s"
   },
   "source": [
    "Next up, we'll need to provide an embedding model that we can use to construct our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP_VDgyx_MPq"
   },
   "source": [
    "Now we can build our in memory QDrant vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spdhnes/AIE6/08_Evaluating_RAG_With_Ragas/.venv/lib/python3.13/site-packages/qdrant_client/http/models/models.py:758: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  description=\"Check that the field is empty, alternative syntax for `is_empty: \\&quot;field_name\\&quot;`\",\n",
      "/home/spdhnes/AIE6/08_Evaluating_RAG_With_Ragas/.venv/lib/python3.13/site-packages/qdrant_client/http/models/models.py:762: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  description=\"Check that the field is null, alternative syntax for `is_null: \\&quot;field_name\\&quot;`\",\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"ai_across_years\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"ai_across_years\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUSCXe7x_h0O"
   },
   "source": [
    "We can now add our documents to our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=split_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBgsT5_m_lOD"
   },
   "source": [
    "Let's define our retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZX68nle_nUm"
   },
   "source": [
    "Now we can produce a node for retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve(state):\n",
    "#   retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "#   return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48-mJHgUsvDG"
   },
   "source": [
    "### Augmented\n",
    "\n",
    "Let's create a simple RAG prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cznU20c0uY9j"
   },
   "source": [
    "### Generation\n",
    "\n",
    "We'll also need an LLM to generate responses - we'll use `gpt-4o-mini` to avoid using the same model as our judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTpV7-b7_44n"
   },
   "source": [
    "Then we can create a `generate` node!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate(state):\n",
    "#   docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "#   messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "#   response = llm.invoke(messages)\n",
    "#   return {\"response\" : response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhD1IxvXu2zX"
   },
   "source": [
    "### Building RAG Graph with LangGraph\n",
    "\n",
    "Let's create some state for our LangGraph RAG graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "  RA_in_propmt = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "  G_in_rag = llm.invoke(RA_in_propmt)\n",
    "  return {\"response\" : G_in_rag.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFEqEQje_--V"
   },
   "source": [
    "Now we can build our simple graph!\n",
    "\n",
    "> NOTE: We're using `add_sequence` since we will always move from retrieval to generation. This is essentially building a chain in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7faffa2a5810>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKX8yupqAIeQ"
   },
   "source": [
    "Let's do a test to make sure it's doing what we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\" : \"How are LLM agents useful?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How are LLM agents useful?',\n",
       " 'context': [Document(metadata={'source': 'data/2024_llms.html', '_id': 'a11f37979cdd4ed7a18828e5bf1595d3', '_collection_name': 'ai_across_years'}, page_content='The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\\n\\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\\n\\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\\n\\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?'),\n",
       "  Document(metadata={'source': 'data/2023_llms.html', '_id': '7e9fbc7d6e8440ab86daff46a7488711', '_collection_name': 'ai_across_years'}, page_content='A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\\n\\nThey’re actually quite easy to build\\n\\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\\n\\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\\n\\nWhat matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\\n\\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.'),\n",
       "  Document(metadata={'source': 'data/2023_llms.html', '_id': 'c21692c974c84a1d9212005f642d65a3', '_collection_name': 'ai_across_years'}, page_content='So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\\n\\nYou can run LLMs on your own devices\\n\\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\\n\\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\\n\\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!'),\n",
       "  Document(metadata={'source': 'data/2024_llms.html', '_id': '76db88cb176f4a7e87da61440bc14b54', '_collection_name': 'ai_across_years'}, page_content='LLMs need better criticism\\n\\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\\n\\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\\n\\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\\n\\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.'),\n",
       "  Document(metadata={'source': 'data/2023_llms.html', '_id': '957a3587c66a4f60ac8559a78eae3d86', '_collection_name': 'ai_across_years'}, page_content='It’s still astonishing to me how effective they are though.\\n\\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\\n\\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\\n\\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\\n\\nHow should we feel about this as software engineers?\\n\\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?')],\n",
       " 'response': \"LLM agents are considered useful for several reasons, despite some skepticism around their utility. Here are the main points derived from the provided context:\\n\\n1. **Acting on Behalf of Users**: Some view LLM agents as digital assistants that can perform tasks or make decisions for users, similar to a travel agent. This model suggests that LLMs can simplify various processes by handling requests autonomously.\\n\\n2. **Tool Utilization**: LLMs can be equipped with access to various tools, enabling them to execute functions in a loop to address specific problems. This capability allows them to be more interactive and effective in problem-solving.\\n\\n3. **Ease of Development**: The surprisingly straightforward process of building LLMs—requiring only a few hundred lines of Python code—means that they are becoming more accessible. While not trivial, the barriers to entry are decreasing compared to earlier expectations.\\n\\n4. **Running on Personal Devices**: Advances have made it feasible to run LLMs on personal devices, making them more accessible to a broader audience. This democratization of technology allows more individuals to leverage LLM capabilities.\\n\\n5. **Effective Code Generation**: LLMs show particular effectiveness in generating code, as they can test and refine their outputs through iterations. This self-correcting capability mitigates some of the issues related to hallucination, which is a significant concern in other applications.\\n\\n6. **Ongoing Critique and Improvement**: There's recognition that while LLMs have potential benefits, they also have significant drawbacks that warrant criticism. Engaging in discussions about these issues is essential for responsible usage and maximizing positive applications while minimizing negative impacts.\\n\\nOverall, while there are concerns regarding LLMs' reliability and the potential for misinformation, their ability to act autonomously, solve problems, and generate code effectively presents valuable opportunities.\"}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LLM agents are considered useful for several reasons, despite some skepticism around their utility. Here are the main points derived from the provided context:\\n\\n1. **Acting on Behalf of Users**: Some view LLM agents as digital assistants that can perform tasks or make decisions for users, similar to a travel agent. This model suggests that LLMs can simplify various processes by handling requests autonomously.\\n\\n2. **Tool Utilization**: LLMs can be equipped with access to various tools, enabling them to execute functions in a loop to address specific problems. This capability allows them to be more interactive and effective in problem-solving.\\n\\n3. **Ease of Development**: The surprisingly straightforward process of building LLMs—requiring only a few hundred lines of Python code—means that they are becoming more accessible. While not trivial, the barriers to entry are decreasing compared to earlier expectations.\\n\\n4. **Running on Personal Devices**: Advances have made it feasible to run LLMs on personal devices, making them more accessible to a broader audience. This democratization of technology allows more individuals to leverage LLM capabilities.\\n\\n5. **Effective Code Generation**: LLMs show particular effectiveness in generating code, as they can test and refine their outputs through iterations. This self-correcting capability mitigates some of the issues related to hallucination, which is a significant concern in other applications.\\n\\n6. **Ongoing Critique and Improvement**: There's recognition that while LLMs have potential benefits, they also have significant drawbacks that warrant criticism. Engaging in discussions about these issues is essential for responsible usage and maximizing positive applications while minimizing negative impacts.\\n\\nOverall, while there are concerns regarding LLMs' reliability and the potential for misinformation, their ability to act autonomously, solve problems, and generate code effectively presents valuable opportunities.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK5gyOlixWr4"
   },
   "source": [
    "## Evaluating the App with Ragas\n",
    "\n",
    "Now we can finally do our evaluation!\n",
    "\n",
    "We'll start by running the queries we generated usign SDG above through our application to get context and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which organizations, including Anthropic, have...</td>\n",
       "      <td>[If you can gather the right data, and afford ...</td>\n",
       "      <td>[We don’t yet know how to build GPT-4 Vibes Ba...</td>\n",
       "      <td>The organizations that have produced large lan...</td>\n",
       "      <td>A year ago, the only organization that had rel...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some challenges associated with evalu...</td>\n",
       "      <td>[This is a huge advantage for open over closed...</td>\n",
       "      <td>[I’m surprised that no-one has beaten the now ...</td>\n",
       "      <td>Some challenges associated with evaluating GPT...</td>\n",
       "      <td>Evaluating GPT-4 is challenging because there ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is AI mean now, is it just LLMs or more, ...</td>\n",
       "      <td>[The two main categories I see are people who ...</td>\n",
       "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
       "      <td>AI today encompasses more than just LLMs (Larg...</td>\n",
       "      <td>AI now often refers to Large Language Models (...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me AI product manager, i look at analytics, i ...</td>\n",
       "      <td>[The top five: ai (342), generativeai (300), l...</td>\n",
       "      <td>[Microsoft over this issue. The 69 page PDF is...</td>\n",
       "      <td>In the provided context, \"Plausible\" refers to...</td>\n",
       "      <td>In the context provided, Plausible refers to P...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key advantages of using synthetic...</td>\n",
       "      <td>[One of the best descriptions I’ve seen of thi...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>The key advantages of using synthetic training...</td>\n",
       "      <td>Synthetic training data offers several direct ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Given the rapid advancements in large language...</td>\n",
       "      <td>[Since then, almost every major LLM (and most ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>The rapid advancements in large language model...</td>\n",
       "      <td>The context reveals that in 2024, significant ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how ai model trainin get more better for envir...</td>\n",
       "      <td>[Law is not ethics. Is it OK to train models o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>When considering the training of AI models, pa...</td>\n",
       "      <td>ai model trainin got better for environment bu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are some of the ethical and environmental...</td>\n",
       "      <td>[Law is not ethics. Is it OK to train models o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>The training and deployment of large language ...</td>\n",
       "      <td>The training and deployment of large language ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Why people keep using ChatGPT even though it s...</td>\n",
       "      <td>[How should we feel about this as software eng...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nI’m surprised that no-one has beat...</td>\n",
       "      <td>People continue to use ChatGPT despite its occ...</td>\n",
       "      <td>People keep using ChatGPT even though it somet...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Considering the dramatic reduction in LLM pric...</td>\n",
       "      <td>[I wrote about this at the time in The killer ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\non a story about the town's histor...</td>\n",
       "      <td>Google’s Gemini 1.5 Flash 8B model exemplifies...</td>\n",
       "      <td>Google’s Gemini 1.5 Flash 8B model exemplifies...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are the key innovations introduced by Gem...</td>\n",
       "      <td>[I wrote about this at the time in The killer ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
       "      <td>Gemini 1.5 Pro, released in February 2024, int...</td>\n",
       "      <td>Gemini 1.5 Pro, also known as Gemini Pro 1.5, ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Why people say ChatGPT is both smart and dumb,...</td>\n",
       "      <td>[But on the other hand, the things you sometim...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nI’m surprised that no-one has beat...</td>\n",
       "      <td>People describe ChatGPT as both smart and dumb...</td>\n",
       "      <td>People say ChatGPT is both smart and dumb beca...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   Which organizations, including Anthropic, have...   \n",
       "1   What are some challenges associated with evalu...   \n",
       "2   what is AI mean now, is it just LLMs or more, ...   \n",
       "3   me AI product manager, i look at analytics, i ...   \n",
       "4   What are the key advantages of using synthetic...   \n",
       "5   Given the rapid advancements in large language...   \n",
       "6   how ai model trainin get more better for envir...   \n",
       "7   What are some of the ethical and environmental...   \n",
       "8   Why people keep using ChatGPT even though it s...   \n",
       "9   Considering the dramatic reduction in LLM pric...   \n",
       "10  What are the key innovations introduced by Gem...   \n",
       "11  Why people say ChatGPT is both smart and dumb,...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [If you can gather the right data, and afford ...   \n",
       "1   [This is a huge advantage for open over closed...   \n",
       "2   [The two main categories I see are people who ...   \n",
       "3   [The top five: ai (342), generativeai (300), l...   \n",
       "4   [One of the best descriptions I’ve seen of thi...   \n",
       "5   [Since then, almost every major LLM (and most ...   \n",
       "6   [Law is not ethics. Is it OK to train models o...   \n",
       "7   [Law is not ethics. Is it OK to train models o...   \n",
       "8   [How should we feel about this as software eng...   \n",
       "9   [I wrote about this at the time in The killer ...   \n",
       "10  [I wrote about this at the time in The killer ...   \n",
       "11  [But on the other hand, the things you sometim...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [We don’t yet know how to build GPT-4 Vibes Ba...   \n",
       "1   [I’m surprised that no-one has beaten the now ...   \n",
       "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
       "3   [Microsoft over this issue. The 69 page PDF is...   \n",
       "4   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "5   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "6   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "7   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "8   [<1-hop>\\n\\nI’m surprised that no-one has beat...   \n",
       "9   [<1-hop>\\n\\non a story about the town's histor...   \n",
       "10  [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
       "11  [<1-hop>\\n\\nI’m surprised that no-one has beat...   \n",
       "\n",
       "                                             response  \\\n",
       "0   The organizations that have produced large lan...   \n",
       "1   Some challenges associated with evaluating GPT...   \n",
       "2   AI today encompasses more than just LLMs (Larg...   \n",
       "3   In the provided context, \"Plausible\" refers to...   \n",
       "4   The key advantages of using synthetic training...   \n",
       "5   The rapid advancements in large language model...   \n",
       "6   When considering the training of AI models, pa...   \n",
       "7   The training and deployment of large language ...   \n",
       "8   People continue to use ChatGPT despite its occ...   \n",
       "9   Google’s Gemini 1.5 Flash 8B model exemplifies...   \n",
       "10  Gemini 1.5 Pro, released in February 2024, int...   \n",
       "11  People describe ChatGPT as both smart and dumb...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   A year ago, the only organization that had rel...   \n",
       "1   Evaluating GPT-4 is challenging because there ...   \n",
       "2   AI now often refers to Large Language Models (...   \n",
       "3   In the context provided, Plausible refers to P...   \n",
       "4   Synthetic training data offers several direct ...   \n",
       "5   The context reveals that in 2024, significant ...   \n",
       "6   ai model trainin got better for environment bu...   \n",
       "7   The training and deployment of large language ...   \n",
       "8   People keep using ChatGPT even though it somet...   \n",
       "9   Google’s Gemini 1.5 Flash 8B model exemplifies...   \n",
       "10  Gemini 1.5 Pro, also known as Gemini Pro 1.5, ...   \n",
       "11  People say ChatGPT is both smart and dumb beca...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TULcXSm0AUe0"
   },
   "source": [
    "Then we can convert that table into a `EvaluationDataset` which will make the process of evaluation smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQibuzfZAbvA"
   },
   "source": [
    "We'll need to select a judge model - in this case we're using the same model that was used to generate our Synthetic Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzHuOZgBAoUU"
   },
   "source": [
    "Next up - we simply evaluate on our desired metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037e3dd7085c44f3a79cc318956087bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[7]: AttributeError('StringIO' object has no attribute 'statements')\n",
      "Exception raised in Job[59]: AttributeError('StringIO' object has no attribute 'statements')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8448, 'faithfulness': 0.8650, 'factual_correctness': 0.6483, 'answer_relevancy': 0.7776, 'context_entity_recall': 0.5215, 'noise_sensitivity_relevant': 0.2188}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZxxr2NqAyBp"
   },
   "source": [
    "## Making Adjustments and Re-Evaluating\n",
    "\n",
    "Now that we've got our baseline - let's make a change and see how the model improves or doesn't improve!\n",
    "\n",
    "> NOTE: This will be using Cohere's Rerank model (which was updated fairly [recently](https://docs.cohere.com/v2/changelog/rerank-v3.5)) - please be sure to [sign-up for an API key!](https://docs.cohere.com/reference/about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass(\"Please enter your Cohere API key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU cohere langchain_cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll first set our retriever to return more documents, which will allow us to take advantage of the reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking, or contextual compression, is a technique that uses a reranker to compress the retrieved documents into a smaller set of documents.\n",
    "\n",
    "This is essentially a slower, more accurate form of semantic similarity that we use on a smaller subset of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "def retrieve_adjusted(state):\n",
    "  compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "  compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever, search_kwargs={\"k\": 5}\n",
    "  )\n",
    "  retrieved_docs = compression_retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply rebuild our graph with the new retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve_adjusted, generate])\n",
    "graph_builder.add_edge(START, \"retrieve_adjusted\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7faff6b06ea0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM agents are useful primarily in two ways: as digital assistants that can act on behalf of users, akin to a travel agent, and as tools that can run loops to solve problems using programming capabilities. The context implies that LLMs excel at writing code, which is a significant application given the simpler grammar rules of programming languages compared to natural languages. However, there is skepticism regarding their overall utility due to concerns about gullibility—LLMs may struggle to distinguish truth from fiction, which limits their effectiveness in making meaningful decisions. Despite the excitement surrounding AI agents, there are few examples of them in production, indicating that their practical implementation is still evolving.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_in_rag = graph.invoke({\"question\" : \"How are LLM agents useful?\"})\n",
    "G_in_rag[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
    "  time.sleep(2) # To try to avoid rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b9b765dc07450a884f333117a9103f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[11]: AttributeError('StringIO' object has no attribute 'statements')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 0.8278, 'faithfulness': 0.7934, 'factual_correctness': 0.6767, 'answer_relevancy': 0.8535, 'context_entity_recall': 0.5353, 'noise_sensitivity_relevant': 0.2071}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset=adjusted_evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question: \n",
    "\n",
    "Which system performed better, on what metrics, and why?\n",
    "\n",
    "base_system: \n",
    "{'context_recall': 0.8448, 'faithfulness': 0.8650, 'factual_correctness': 0.6483, 'answer_relevancy': 0.7776, 'context_entity_recall': 0.5215, 'noise_sensitivity_relevant': 0.2188}\n",
    "\n",
    "adjusted_system:\n",
    "{'context_recall': 0.8278, 'faithfulness': 0.7934, 'factual_correctness': 0.6767, 'answer_relevancy': 0.8535, 'context_entity_recall': 0.5353, 'noise_sensitivity_relevant': 0.2071}\n",
    "\n",
    "\n",
    "We made a change and some numbers affected positively. However, why it changed gear to negative direction  for context recal and faithfullness even if the brain is bigger and retriver is coherent!!\n",
    "\n",
    "Base system is better at faithfully recalling the original context and being faithful to the source.\n",
    "\n",
    "Adjusted system improved factual correctness (more accurate answers) and answer relevancy (more relevant answers), but sacrificed some faithfulness and context recall.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
